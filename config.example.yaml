# config.example.yaml - Example configuration for RT Transcription System
# Copy this file to config.yaml and update with your actual values

# Multi-Process Architecture Settings
architecture:
  use_tmux: true              # Use tmux for window management
  max_memory_mb: 500          # Maximum memory usage before cleanup
  process_restart_delay: 5    # Seconds to wait before restarting failed process
  health_check_interval: 30   # Seconds between health checks
  transcript_buffer_size: 100 # Maximum segments to keep in memory

audio:
  sample_rate: 16000   # Common sample rate for speech recognition
  channels: 1          # Mono audio is typical for transcription

chunking:
  mode: "fixed"        # "fixed" or "vad" (Voice Activity Detection)
                       # "vad" requires PyTorch & TorchAudio: pip install torch torchaudio
  chunk_seconds: 20    # Duration of each audio chunk in seconds (if mode is "fixed")
  vad_silence_seconds: 1.0 # Trailing silence to include after speech in "vad" mode (s)
  max_chunk_seconds: 30    # Hard cap for VAD recording length (s)

ml:
  source_language: "zh"  # Language code for transcription (e.g., "en", "es", "zh")
                         # Set to None for auto-detection by Whisper (if supported by model)
  model_path: "mlx-community/whisper-tiny-mlx" # Path to local MLX Whisper model directory
                                               # or Hugging Face repo name for MLX models.
                                               # Examples:
                                               # - "mlx_models/whisper-small-weights" (if downloaded locally)
                                               # - "mlx-community/whisper-small-mlx"
                                               # - "mlx-community/whisper-base-mlx"
                                               # - "mlx-community/whisper-large-v3-mlx"
monitoring:
  interval_minutes: 4
  lookback_minutes: 5
  
note_taking:
  depth_level: "standard"           # minimal, standard, detailed, comprehensive
  analysis_level: "light"           # none, light, moderate, heavy
  fact_focus: "high"               # low, medium, high
  interpretation_focus: "minimal"   # minimal, balanced, heavy
  
capture_style:
  prioritize_facts: true           # what was said/done vs why
  include_quotes: true             # verbatim statements
  include_behaviors: true          # actions, reactions
  include_context: true            # situational details
  include_emotions: false          # emotional interpretation
  include_implications: false      # business/research implications
  
format:
  bullet_style: "factual"          # factual, analytical, mixed
  timestamp_precision: "minute"    # second, minute, segment
  quote_length: "brief"            # brief, full, context
  
files:
  transcript_path: "./rt_transcript"  # Directory for session transcripts
  processed_log: "./processed_timestamps.log"
  notes_log: "./research_notes.log"
  
# DeepSeek API Configuration (for AI note generation)
deepseek:
  api_key: "YOUR_DEEPSEEK_API_KEY_HERE"  # Replace with your actual API key
  base_url: "https://api.deepseek.com"
  model: "deepseek-chat"
  max_tokens_completion: 1500
  max_retries: 3
  timeout_connect: 15.0
  timeout_read: 60.0
  timeout_write: 10.0
  timeout_pool: 5.0

# Legacy OpenAI config (optional - not used in current implementation)
openai:
  api_key: "YOUR_OPENAI_API_KEY_HERE"
  model: "gpt-4"
  max_tokens: 700
  
output:
  new_terminal: true
  log_to_file: true
  display_format: "clean"          # clean, detailed, timestamp_heavy